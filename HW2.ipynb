{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TheoryofML_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Theory of ML - Homework 2 "
      ],
      "metadata": {
        "id": "pjvWbVyt5cQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 1\n",
        "(Loss minimization vs. misclassification)\n",
        "Consider the case d = 1 and implement gradient descent for minimizing L(w). First consider a simple “well separable” case with 100 data points (and labels) as follows:\n",
        "(−50,−1), (−49,−1), ..., (−1,−1), (1,1), (2,1), ..., (50,1). Initialize w = −1 and run 100 iterations of gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "q0OCY-zyVjTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import exp\n",
        "import math"
      ],
      "metadata": {
        "id": "CqzyZB8fWV3_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = []\n",
        "for i in range(1,51):\n",
        "  points.append((i,1))\n",
        "  points.append((-i,-1))"
      ],
      "metadata": {
        "id": "HoShbzVoZzRP"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_val(w,x,y):\n",
        "  func = math.exp(-1 * x * y * w)\n",
        "  grad = (-1 * x * y) * func / (1+func)\n",
        "  return grad"
      ],
      "metadata": {
        "id": "_Wz3lnp6rsbE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(points):\n",
        "    w = -1\n",
        "    loss=0\n",
        "    L = 0.05\n",
        "    epochs = 100\n",
        "\n",
        "    for i in range(100):\n",
        "      avg_grad = 0\n",
        "      for x in points:\n",
        "        avg_grad = gradient_val(x[0],x[1],w)\n",
        "        w = w - (0.1)*avg_grad\n",
        "    return w"
      ],
      "metadata": {
        "id": "CApNhhzXZBS0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = []\n",
        "for i in range(1,51):\n",
        "  points.append((i,1))\n",
        "  points.append((-i,-1))\n",
        "\n",
        "print(logistic_regression(points))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4MMpS_0PZwt",
        "outputId": "11bc0501-37c1-49e7-e342-f77fa95e724d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.3263570509839837e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose we “corrupt” some labels. Specifically, take the 10 points with the highest abso- lute value of x (i.e., 50, −50, 49, −49, ..., 46, −46) and reverse the sign of their labels. Now show the result of performing gradient descent, and interpret your result. (Note that we only corrupted 10% of the labels.)"
      ],
      "metadata": {
        "id": "siF0Knx706fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points_corrupt = []\n",
        "for i in range(1,51):\n",
        "  if i<46:\n",
        "    points.append((i,1))\n",
        "    points.append((-i,-1))\n",
        "  else:\n",
        "    points.append((-i,1))\n",
        "    points.append((i,-1))"
      ],
      "metadata": {
        "id": "P5UGP4_oxjVA"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logistic_regression(points_corrupt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUbNoL5qzhRJ",
        "outputId": "3f68af02-5e2a-4c4d-8218-5d2178cf6971"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now show the result of performing gradient descent, and interpret your result. \n",
        "\n",
        "Answer: Weirdly so, the loss function is not reducing for the corrupt condition."
      ],
      "metadata": {
        "id": "HsfnjnKP0pFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem3"
      ],
      "metadata": {
        "id": "qeLXhEk25yls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a ‘random’ dataset for this problem as follows. Set n = 500, m = 2n, and let A have entries that are random in the interval [−1, 1]. Now choose some “hidden” vector x∗ (entries again random in [−1, 1]), and define b as Ax∗ + η, where η is a vector whose coordinates are Gaussian with mean 0 and variance 0.5."
      ],
      "metadata": {
        "id": "FosppVNi53n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "RssypUyI8Lko"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.uniform(-1.0,1.0,size=(1000,500))\n",
        "x = np.random.uniform(-1.0,1.0,size=(500,1))\n",
        "n = np.random.normal(loc=0.0, scale=math.sqrt(0.5), size=1000)"
      ],
      "metadata": {
        "id": "j_o2OdXE6H9i"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n.shape,x.shape,A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc8zIuZj_mhc",
        "outputId": "d9a3a7a8-3131-4b12-d0ec-2c49cac08420"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000,), (500, 1), (1000, 500))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.zeros((1000,1))\n",
        "b = np.matmul(A,x)+n[0]\n",
        "b.shape\n",
        "# print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUI-UjaD_DlP",
        "outputId": "5af682db-936a-4556-d7d6-90de674d565a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gradient descent with a constant stepsize (say 1/10) starting with x0 = 0, and report the function value and the distance to the ‘hidden’ x∗ after 50 steps.\n"
      ],
      "metadata": {
        "id": "g5cvtFNA-GZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "At= np.transpose(A)"
      ],
      "metadata": {
        "id": "3YP4TRezCBhH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(inp):\n",
        "  tmp = np.dot(At,A)\n",
        "  tmp1 = np.dot(tmp,inp)\n",
        "  tmp2 = np.dot(At,b)\n",
        "  return 2*np.subtract(tmp1,tmp2)"
      ],
      "metadata": {
        "id": "JTT0isl_BLfk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent( start, lr, iter):\n",
        "    vector = start\n",
        "    for _ in range(iter):\n",
        "        diff = -lr * gradient(vector)\n",
        "        vector += diff\n",
        "    return vector"
      ],
      "metadata": {
        "id": "n7iZApoYBLsc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = [0 for i in range(len(x))]"
      ],
      "metadata": {
        "id": "wf_ABZdQBL4F"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_val= gradient_descent(start,0.1,50)"
      ],
      "metadata": {
        "id": "YsNHggCTBL9M"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = np.linalg.norm(x-func_val)\n",
        "distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EJ46L8BMBZ",
        "outputId": "646d43e0-3ecc-42a4-fe56-0cc95d61d37c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6416516334256747e+115"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is well known that least squares regression has a closed form, given by x∗ = (AT A)−1AT b. Using a numerical library for the inverse, compute x∗ using this formula. "
      ],
      "metadata": {
        "id": "IU-DDhA_BIlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form():\n",
        "  t = np.dot(At,A)\n",
        "  t_inv = np.linalg.inv(t)\n",
        "  t1 = np.dot(t_inv,At)\n",
        "  return np.dot(t1,b) "
      ],
      "metadata": {
        "id": "0e1lwKRZ63xJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_f = closed_form()\n",
        "print(np.linalg.norm(x-c_f))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRmg6HtEqbb",
        "outputId": "9ff4c4ab-b13f-4ac4-f59d-bd412b331b8e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6147711622958044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the running time of this method to that of gradient descent in part (d).\n",
        "\n",
        "Answer: The closed form was faster than the individual gradient calculations. "
      ],
      "metadata": {
        "id": "sLjvtsfKE8QW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4"
      ],
      "metadata": {
        "id": "8h_Kj4UrIS7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x,a):\n",
        "  su = 0\n",
        "  for i in range(200):\n",
        "    su += (x-a)\n",
        "  return su/200"
      ],
      "metadata": {
        "id": "hZ4BJFi2Ezt2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(x,y):\n",
        "  for i in range(1,201):\n",
        "    if i<=100:\n",
        "      a = i/200\n",
        "      b = -1\n",
        "    else:\n",
        "      a = (i-100)/200\n",
        "      b = 1\n",
        "    x = x - (0.1)*(gradient(x,a))\n",
        "    y = y - (0.1)*(gradient(y,b))\n",
        "  return (x,y)"
      ],
      "metadata": {
        "id": "woHZYXSoXBDC"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_t(x,y):\n",
        "  for i in range(1,201):\n",
        "    if i<=100:\n",
        "      a = i/200\n",
        "      b = -1\n",
        "    else:\n",
        "      a = (i-100)/200\n",
        "      b = 1\n",
        "    x = x - (0.1/(i+1))*(gradient(x,a))\n",
        "    y = y - (0.1/(i+1))*(gradient(y,b))\n",
        "  return (x,y)"
      ],
      "metadata": {
        "id": "cSZaDaq-XDot"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_sqrt(x,y):\n",
        "  for i in range(1,201):\n",
        "    if i<=100:\n",
        "      a = i/200\n",
        "      b = -1\n",
        "    else:\n",
        "      a = (i-100)/200\n",
        "      b = 1\n",
        "    x = x - (0.1/math.sqrt(i+1))*(gradient(x,a))\n",
        "    y = y - (0.1/math.sqrt(i+1))*(gradient(y,b))\n",
        "  return (x,y)"
      ],
      "metadata": {
        "id": "hacGU2ABXLiA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd(1,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oljtGz9YXf1u",
        "outputId": "da25bd4d-0b39-4354-bb62-41aab343214d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.4550132814366996, 0.9999468786132406)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_t(1,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc_su0ByXibm",
        "outputId": "40f1003f-7fa1-49d2-8ee7-2a7268300ea4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.6678371497687322, 0.3559253449099471)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_sqrt(1,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA0F7KkFXsFN",
        "outputId": "73258113-bd10-44dd-de19-81c3861cd79e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.3212585163101469, 0.2716444055266952)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the learning rate is reduced (by making it time dependent), we see a better optimization to minima. "
      ],
      "metadata": {
        "id": "jPjQJe0CX8RS"
      }
    }
  ]
}