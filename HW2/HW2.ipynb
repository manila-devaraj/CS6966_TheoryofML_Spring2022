{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TheoryofML_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Theory of ML - Homework 2 "
      ],
      "metadata": {
        "id": "pjvWbVyt5cQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem1\n",
        "(Loss minimization vs. misclassification)\n",
        "Consider the case d = 1 and implement gradient descent for minimizing L(w). First consider a simple “well separable” case with 100 data points (and labels) as follows:\n",
        "(−50,−1), (−49,−1), ..., (−1,−1), (1,1), (2,1), ..., (50,1). Initialize w = −1 and run 100 iterations of gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "q0OCY-zyVjTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import exp\n",
        "import math"
      ],
      "metadata": {
        "id": "CqzyZB8fWV3_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_1(w, x,y):\n",
        "  grad = 0\n",
        "  denom = (1 + math.exp(-1 * x * y * w))\n",
        "  grad += (-1 * x * y) * math.exp(-1 * x * y * w) / denom\n",
        "  return grad"
      ],
      "metadata": {
        "id": "SxGhYTUBobCl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fucntion(w, x,y):\n",
        "  loss = 0\n",
        "  exp_val = (-1 * x * y * w)\n",
        "  loss += math.log(1 + math.exp(-1 * x * y * w))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "y8Dypqnuo3Qr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(start, lr, n_iter, datax,datay):\n",
        "  vector = start\n",
        "  for i in range(n_iter):\n",
        "    for x in datax:\n",
        "      for y in datay:\n",
        "        diff = -lr * gradient_1(vector, x,y)\n",
        "        # print(\"diff\", diff)\n",
        "        vector += diff\n",
        "        # print(\"loss at {}:{}\".format(i, loss_fucntion(vector, x[0],x[1])))\n",
        "  return vector"
      ],
      "metadata": {
        "id": "meomlCHto8FP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.linspace(-50, -1, 50)\n",
        "x2 = np.linspace(1, 50, 50)\n",
        "x= np.concatenate((x1,x2))\n",
        "y1= np.full(shape=50,fill_value=-1,dtype=np.int)\n",
        "y2= np.full(shape=50,fill_value=1,dtype=np.int)\n",
        "y= np.concatenate((y1,y2))\n",
        "plt.plot(x,y,'o')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "G6iulYx1A1-H",
        "outputId": "cf9dc1e1-a961-44ef-e9c5-de520e3c13a6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "omega = gradient_descent(start=-1, lr=0.0001, n_iter=100, datax=x,datay=y)"
      ],
      "metadata": {
        "id": "M4MMpS_0PZwt"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final omega:\",omega)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoudeeS5rdP6",
        "outputId": "9c1d0b9a-ecc0-4680-a613-37c04641fb48"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final omega: 0.04300309197099269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose we “corrupt” some labels. Specifically, take the 10 points with the highest abso- lute value of x (i.e., 50, −50, 49, −49, ..., 46, −46) and reverse the sign of their labels. Now show the result of performing gradient descent, and interpret your result. (Note that we only corrupted 10% of the labels.)"
      ],
      "metadata": {
        "id": "siF0Knx706fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_corrupt= x\n",
        "y1= np.full(shape=5,fill_value=1,dtype=np.int)\n",
        "y2= np.full(shape=45,fill_value=-1,dtype=np.int)\n",
        "y3= np.full(shape=45,fill_value=1,dtype=np.int)\n",
        "y4=np.full(shape=5,fill_value=-1,dtype=np.int)\n",
        "y_corrupt= np.concatenate((y1,y2,y3,y4))\n",
        "plt.plot(x_corrupt,y_corrupt,'o')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "P5UGP4_oxjVA",
        "outputId": "a14dc427-0c5d-446b-b395-74cd2242d5c5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "omega_corrupted = gradient_descent(start=-1, lr=0.0001, n_iter=100, datax=x_corrupt,datay=y_corrupt)"
      ],
      "metadata": {
        "id": "GUbNoL5qzhRJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final omega with corrupted points:\",omega_corrupted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEgu1dzUrl33",
        "outputId": "cff22d7c-8291-4232-be2e-504c379c0245"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final omega with corrupted points: 0.01962220421419022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now show the result of performing gradient descent, and interpret your result. \n",
        "\n",
        "Answer: Gradient descent is optimizing to local minima in both the cases. "
      ],
      "metadata": {
        "id": "HsfnjnKP0pFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem3"
      ],
      "metadata": {
        "id": "qeLXhEk25yls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a ‘random’ dataset for this problem as follows. Set n = 500, m = 2n, and let A have entries that are random in the interval [−1, 1]. Now choose some “hidden” vector x∗ (entries again random in [−1, 1]), and define b as Ax∗ + η, where η is a vector whose coordinates are Gaussian with mean 0 and variance 0.5."
      ],
      "metadata": {
        "id": "FosppVNi53n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "RssypUyI8Lko"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(M,N):\n",
        "  A = np.random.uniform(-1.0,1.0,size=(M,N))\n",
        "  x = np.random.uniform(-1.0,1.0,size=(N,1))\n",
        "  n = np.random.normal(loc=0.0, scale=math.sqrt(0.5), size=1000).reshape(M,1)\n",
        "  b = np.zeros((1000,1))\n",
        "  b = np.dot(A,x)+n\n",
        "  return A,x,n,b"
      ],
      "metadata": {
        "id": "plqC_9wV_E5v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N=500\n",
        "M=2*N\n",
        "A,x,n,b=create_data(M,N)"
      ],
      "metadata": {
        "id": "j_o2OdXE6H9i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n.shape,x.shape,A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc8zIuZj_mhc",
        "outputId": "85a6be1a-36c3-4eed-9968-022e7d35ee5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 1), (500, 1), (1000, 500))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gradient descent with a constant stepsize (say 1/10) starting with x0 = 0, and report the function value and the distance to the ‘hidden’ x∗ after 50 steps.\n"
      ],
      "metadata": {
        "id": "g5cvtFNA-GZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_3(x):\n",
        "  return 2*np.dot(np.transpose(A), np.subtract(np.dot(A,x),b))"
      ],
      "metadata": {
        "id": "JTT0isl_BLfk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent( start, lr, iter):\n",
        "    weight = start\n",
        "    for _ in range(iter):\n",
        "      diff = -lr * gradient_3(weight)\n",
        "      # print(diff)\n",
        "      weight+= diff\n",
        "    return weight"
      ],
      "metadata": {
        "id": "n7iZApoYBLsc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.zeros(x.shape)"
      ],
      "metadata": {
        "id": "wf_ABZdQBL4F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_zero=time.time()\n",
        "func_val= gradient_descent(start,0.1,50)\n",
        "time_gradient=time.time()\n",
        "time_gradient-=time_zero\n",
        "time_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsNHggCTBL9M",
        "outputId": "4c83c18a-415c-4d51-cb4a-ddf322a20a60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12498998641967773"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the gradient keeps oscillating. The reason is that we started at (0,0) which is already close to the minimum and in the first iteration, since the gradient is huge(because of large learning rate) it takes omega to a far away point which might take more iterations to optimize to minimum point. "
      ],
      "metadata": {
        "id": "P6tEmNVwXmDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distance of gradient descent:\",np.linalg.norm(x-func_val))\n",
        "print(\"Time for gradient descent:\",time_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EJ46L8BMBZ",
        "outputId": "0ef4b4e4-abf0-4719-b0ea-c90b0c66e62d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance of gradient descent: 4.6592214245212325e+113\n",
            "Time for gradient descent: 0.12498998641967773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is well known that least squares regression has a closed form, given by x∗ = (AT A)−1AT b. Using a numerical library for the inverse, compute x∗ using this formula. "
      ],
      "metadata": {
        "id": "IU-DDhA_BIlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form(A,b):\n",
        "  return np.dot(np.dot(np.linalg.inv(np.dot(A.transpose(),A)),A.transpose()),b) "
      ],
      "metadata": {
        "id": "0e1lwKRZ63xJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_zero_closed=time.time()\n",
        "cf = closed_form(A,b)\n",
        "time_closed_form = time.time()\n",
        "time_closed_form-=time_zero_closed\n",
        "print(\"Distance of closed form:\",np.linalg.norm(x-cf))\n",
        "print(\"Time closed form:\",time_closed_form)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRmg6HtEqbb",
        "outputId": "5d6c0b9f-6b9f-4b8a-9527-783ab575f378"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance of closed form: 1.172030458408197\n",
            "Time closed form: 0.2030324935913086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the running time of this method to that of gradient descent in part (d).\n"
      ],
      "metadata": {
        "id": "sLjvtsfKE8QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "running_time_difference= time_gradient-time_closed_form\n",
        "running_time_difference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqqqZZiDC00s",
        "outputId": "1396752f-78db-477e-b1aa-3800c7ef230b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.07804250717163086"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer: The running time of gradient descent is faster than the closed form computation  "
      ],
      "metadata": {
        "id": "gv_NGRjZC4p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4"
      ],
      "metadata": {
        "id": "8h_Kj4UrIS7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    data = []\n",
        "    for i in range(100):\n",
        "        a = (i/100)\n",
        "        b = -1\n",
        "        tup = (a, b)\n",
        "        data.append(tup)\n",
        "    for i in range(100):\n",
        "        a = (i/100)\n",
        "        b = 1\n",
        "        tup = (a, b)\n",
        "        data.append(tup)\n",
        "    return data"
      ],
      "metadata": {
        "id": "v-HnyCOPw1FA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_4(x, y, a, b, t,grad_type):\n",
        "  if type==\"sgd\":\n",
        "    eta = 0.1\n",
        "  elif type==\"t+1\":\n",
        "    eta= 0.1/(t+1)\n",
        "  else: \n",
        "    eta= 0.1/math.sqrt(t+1)\n",
        "  x_t = x * (1 - (2 * eta)) + 2 * eta * a\n",
        "  y_t = y * (1 - (2 * eta)) + 2 * eta * b\n",
        "  return x_t,y_t"
      ],
      "metadata": {
        "id": "hZ4BJFi2Ezt2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x, y, data):\n",
        "    value = 0\n",
        "    for i in range(len(data)):\n",
        "        value += pow(x - data[i][0], 2) + pow(y - data[i][1], 2)\n",
        "    value = value / 400\n",
        "    return value"
      ],
      "metadata": {
        "id": "ocPiFVPdxBeK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(data, n_iter,grad_type):\n",
        "    x = 1\n",
        "    y = 1\n",
        "    print(\"initial loss:\", loss_function(x, y, data))\n",
        "    for i in range(n_iter):\n",
        "        random_num = random.randrange(0, 200)\n",
        "        a_i = data[random_num][0]\n",
        "        b_i = data[random_num][1]\n",
        "        grad = gradient_4(x, y, a_i, b_i, i,grad_type)\n",
        "        x = grad[0]\n",
        "        y = grad[1]\n",
        "        # print(\"loss at iter {}: {}\".format(i, loss_function(grad[0], grad[1], data)))\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "7rmDLvw7tr8T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=get_data()"
      ],
      "metadata": {
        "id": "_-qtfyBKuuMs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"sgd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oljtGz9YXf1u",
        "outputId": "136b9821-e529-45f5-bca1-327353067392"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5301493968392726, 0.24103243050221873)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"t+1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGTLWuEUwGEo",
        "outputId": "44819424-e452-422a-995a-384c61f4e1cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5388909707276541, -0.0033693766499475093)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"sqrtt+1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyv4CJxJxSlK",
        "outputId": "5bef0f78-8799-4d31-d98f-49c4ec918cfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5048979874989314, 0.09840357617915674)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the learning rate is reduced (by making it time dependent), we see a slower optimization to minima. "
      ],
      "metadata": {
        "id": "jPjQJe0CX8RS"
      }
    }
  ]
}