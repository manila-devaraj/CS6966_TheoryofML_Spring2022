{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TheoryofML_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Theory of ML - Homework 2 "
      ],
      "metadata": {
        "id": "pjvWbVyt5cQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem1\n",
        "(Loss minimization vs. misclassification)\n",
        "Consider the case d = 1 and implement gradient descent for minimizing L(w). First consider a simple “well separable” case with 100 data points (and labels) as follows:\n",
        "(−50,−1), (−49,−1), ..., (−1,−1), (1,1), (2,1), ..., (50,1). Initialize w = −1 and run 100 iterations of gradient descent.\n",
        "\n"
      ],
      "metadata": {
        "id": "q0OCY-zyVjTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import exp\n",
        "import math"
      ],
      "metadata": {
        "id": "CqzyZB8fWV3_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(w, x,y):\n",
        "  grad = 0\n",
        "  denom = (1 + math.exp(-1 * x * y * w))\n",
        "  grad += (-1 * x * y) * math.exp(-1 * x * y * w) / denom\n",
        "  return grad"
      ],
      "metadata": {
        "id": "SxGhYTUBobCl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fucntion(w, x,y):\n",
        "  loss = 0\n",
        "  exp_val = (-1 * x * y * w)\n",
        "  loss += math.log(1 + math.exp(-1 * x * y * w))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "y8Dypqnuo3Qr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(start, lr, n_iter, points):\n",
        "  vector = start\n",
        "  for i in range(n_iter):\n",
        "    for x in points:\n",
        "      diff = -lr * gradient(vector, x[0],x[1])\n",
        "      # print(\"diff\", diff)\n",
        "      vector += diff\n",
        "      # print(\"loss at {}:{}\".format(i, loss_fucntion(vector, x[0],x[1])))\n",
        "  return vector"
      ],
      "metadata": {
        "id": "meomlCHto8FP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = []\n",
        "for i in range(1,51):\n",
        "  points.append((i,1))\n",
        "  points.append((-i,-1))\n",
        "\n",
        "omega = gradient_descent(start=-1, lr=0.0001, n_iter=100, points=points)\n"
      ],
      "metadata": {
        "id": "M4MMpS_0PZwt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final omega:\",omega)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoudeeS5rdP6",
        "outputId": "4958a4c1-3b75-467f-9ff8-668209f5556a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final omega: 0.35933808666389344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now suppose we “corrupt” some labels. Specifically, take the 10 points with the highest abso- lute value of x (i.e., 50, −50, 49, −49, ..., 46, −46) and reverse the sign of their labels. Now show the result of performing gradient descent, and interpret your result. (Note that we only corrupted 10% of the labels.)"
      ],
      "metadata": {
        "id": "siF0Knx706fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points_corrupt = []\n",
        "for i in range(1,51):\n",
        "  if i>46:\n",
        "    points_corrupt.append((i,1))\n",
        "    points_corrupt.append((-i,-1))\n",
        "  else:\n",
        "    points_corrupt.append((-i,1))\n",
        "    points_corrupt.append((i,-1))"
      ],
      "metadata": {
        "id": "P5UGP4_oxjVA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "omega_corrupted = gradient_descent(start=-1, lr=0.0001, n_iter=100, points=points_corrupt)"
      ],
      "metadata": {
        "id": "GUbNoL5qzhRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final omega with corrupted points:\",omega_corrupted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEgu1dzUrl33",
        "outputId": "f5bcdc33-0fde-4be8-a6a9-7b56abe7b00b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final omega with corrupted points: -0.032099959259394434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now show the result of performing gradient descent, and interpret your result. \n",
        "\n",
        "Answer: Gradient descent is optimizing to local minima in both the cases. "
      ],
      "metadata": {
        "id": "HsfnjnKP0pFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem3"
      ],
      "metadata": {
        "id": "qeLXhEk25yls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a ‘random’ dataset for this problem as follows. Set n = 500, m = 2n, and let A have entries that are random in the interval [−1, 1]. Now choose some “hidden” vector x∗ (entries again random in [−1, 1]), and define b as Ax∗ + η, where η is a vector whose coordinates are Gaussian with mean 0 and variance 0.5."
      ],
      "metadata": {
        "id": "FosppVNi53n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "RssypUyI8Lko"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(M,N):\n",
        "  A = np.random.uniform(-1.0,1.0,size=(M,N))\n",
        "  x = np.random.uniform(-1.0,1.0,size=(N,1))\n",
        "  n = np.random.normal(loc=0.0, scale=math.sqrt(0.5), size=1000).reshape(M,1)\n",
        "  b = np.zeros((1000,1))\n",
        "  b = np.dot(A,x)+n\n",
        "  return A,x,n,b"
      ],
      "metadata": {
        "id": "plqC_9wV_E5v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N=500\n",
        "M=2*N\n",
        "A,x,n,b=create_data(M,N)"
      ],
      "metadata": {
        "id": "j_o2OdXE6H9i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n.shape,x.shape,A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc8zIuZj_mhc",
        "outputId": "85a6be1a-36c3-4eed-9968-022e7d35ee5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 1), (500, 1), (1000, 500))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gradient descent with a constant stepsize (say 1/10) starting with x0 = 0, and report the function value and the distance to the ‘hidden’ x∗ after 50 steps.\n"
      ],
      "metadata": {
        "id": "g5cvtFNA-GZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x):\n",
        "  return 2*np.dot(np.transpose(A), np.subtract(np.dot(A,x),b))"
      ],
      "metadata": {
        "id": "JTT0isl_BLfk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent( start, lr, iter):\n",
        "    weight = start\n",
        "    for _ in range(iter):\n",
        "      diff = -lr * gradient(weight)\n",
        "      # print(diff)\n",
        "      weight+= diff\n",
        "    return weight"
      ],
      "metadata": {
        "id": "n7iZApoYBLsc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = np.zeros(x.shape)"
      ],
      "metadata": {
        "id": "wf_ABZdQBL4F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_zero=time.time()\n",
        "func_val= gradient_descent(start,0.1,50)\n",
        "time_gradient=time.time()\n",
        "time_gradient-=time_zero\n",
        "time_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsNHggCTBL9M",
        "outputId": "4c83c18a-415c-4d51-cb4a-ddf322a20a60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12498998641967773"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the gradient keeps oscillating. The reason is that we started at (0,0) which is already close to the minimum and in the first iteration, since the gradient is huge(because of large learning rate) it takes omega to a far away point which might take more iterations to optimize to minimum point. "
      ],
      "metadata": {
        "id": "P6tEmNVwXmDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Distance of gradient descent:\",np.linalg.norm(x-func_val))\n",
        "print(\"Time for gradient descent:\",time_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EJ46L8BMBZ",
        "outputId": "0ef4b4e4-abf0-4719-b0ea-c90b0c66e62d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance of gradient descent: 4.6592214245212325e+113\n",
            "Time for gradient descent: 0.12498998641967773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is well known that least squares regression has a closed form, given by x∗ = (AT A)−1AT b. Using a numerical library for the inverse, compute x∗ using this formula. "
      ],
      "metadata": {
        "id": "IU-DDhA_BIlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form(A,b):\n",
        "  return np.dot(np.dot(np.linalg.inv(np.dot(A.transpose(),A)),A.transpose()),b) "
      ],
      "metadata": {
        "id": "0e1lwKRZ63xJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_zero_closed=time.time()\n",
        "cf = closed_form(A,b)\n",
        "time_closed_form = time.time()\n",
        "time_closed_form-=time_zero_closed\n",
        "print(\"Distance of closed form:\",np.linalg.norm(x-cf))\n",
        "print(\"Time closed form:\",time_closed_form)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRmg6HtEqbb",
        "outputId": "5d6c0b9f-6b9f-4b8a-9527-783ab575f378"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance of closed form: 1.172030458408197\n",
            "Time closed form: 0.2030324935913086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the running time of this method to that of gradient descent in part (d).\n"
      ],
      "metadata": {
        "id": "sLjvtsfKE8QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "running_time_difference= time_gradient-time_closed_form\n",
        "running_time_difference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqqqZZiDC00s",
        "outputId": "1396752f-78db-477e-b1aa-3800c7ef230b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.07804250717163086"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer: The running time of gradient descent is faster than the closed form computation  "
      ],
      "metadata": {
        "id": "gv_NGRjZC4p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4"
      ],
      "metadata": {
        "id": "8h_Kj4UrIS7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    data = []\n",
        "    for i in range(100):\n",
        "        a = (i/100)\n",
        "        b = -1\n",
        "        tup = (a, b)\n",
        "        data.append(tup)\n",
        "    for i in range(100):\n",
        "        a = (i/100)\n",
        "        b = 1\n",
        "        tup = (a, b)\n",
        "        data.append(tup)\n",
        "    return data"
      ],
      "metadata": {
        "id": "v-HnyCOPw1FA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x, y, a, b, t,grad_type):\n",
        "  if type==\"sgd\":\n",
        "    eta = 0.1\n",
        "  elif type==\"t+1\":\n",
        "    eta= 0.1/(t+1)\n",
        "  else: \n",
        "    eta= 0.1/math.sqrt(t+1)\n",
        "  x_t = x * (1 - (2 * eta)) + 2 * eta * a\n",
        "  y_t = y * (1 - (2 * eta)) + 2 * eta * b\n",
        "  return x_t,y_t"
      ],
      "metadata": {
        "id": "hZ4BJFi2Ezt2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(x, y, data):\n",
        "    value = 0\n",
        "    for i in range(len(data)):\n",
        "        value += pow(x - data[i][0], 2) + pow(y - data[i][1], 2)\n",
        "    value = value / 400\n",
        "    return value"
      ],
      "metadata": {
        "id": "ocPiFVPdxBeK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(data, n_iter,grad_type):\n",
        "    x = 1\n",
        "    y = 1\n",
        "    print(\"initial loss:\", loss_function(x, y, data))\n",
        "    for i in range(n_iter):\n",
        "        random_num = random.randrange(0, 200)\n",
        "        a_i = data[random_num][0]\n",
        "        b_i = data[random_num][1]\n",
        "        grad = gradient(x, y, a_i, b_i, i,grad_type)\n",
        "        x = grad[0]\n",
        "        y = grad[1]\n",
        "        # print(\"loss at iter {}: {}\".format(i, loss_function(grad[0], grad[1], data)))\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "7rmDLvw7tr8T"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=get_data()"
      ],
      "metadata": {
        "id": "_-qtfyBKuuMs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"sgd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oljtGz9YXf1u",
        "outputId": "136b9821-e529-45f5-bca1-327353067392"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5301493968392726, 0.24103243050221873)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"t+1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGTLWuEUwGEo",
        "outputId": "44819424-e452-422a-995a-384c61f4e1cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5388909707276541, -0.0033693766499475093)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_descent(data,200,grad_type=\"sqrtt+1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyv4CJxJxSlK",
        "outputId": "5bef0f78-8799-4d31-d98f-49c4ec918cfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial loss: 1.169175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5048979874989314, 0.09840357617915674)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the learning rate is reduced (by making it time dependent), we see a slower optimization to minima. "
      ],
      "metadata": {
        "id": "jPjQJe0CX8RS"
      }
    }
  ]
}